{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS3_JFelipeMontanoCampos.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvqXPucJbWv/I+x39aEZSO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipemontano10/DeepLearning/blob/master/PS3_JFelipeMontanoCampos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXI5IF9-8XaV"
      },
      "source": [
        "### ECE 685D - Fall 2020\n",
        "## Problem Set 3 \n",
        "\n",
        "> J. Felipe Montano-Campos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBBOnwiF8Wjt"
      },
      "source": [
        "**Problem 3: Binary Classification with Generalized Linear Models**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rMDvYhl856J"
      },
      "source": [
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYG0rwoP_YuW",
        "outputId": "9123c309-a165-45ff-87a1-6ba76799e14b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4f9103da-3ada-4e0f-a7ef-490e97a27c0e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4f9103da-3ada-4e0f-a7ef-490e97a27c0e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving breast_cancer.csv to breast_cancer.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-v0M0QC8_zM"
      },
      "source": [
        "#Data Preparation \n",
        "cancer_data = pd.read_csv(\"breast_cancer.csv\") \n",
        "\n",
        "targets = cancer_data[\"diagnosis\"]\n",
        "targets = targets.replace({\"B\": 0, \"M\": 1})\n",
        "\n",
        "features = ['radius_mean', 'texture_mean', 'smoothness_mean',\n",
        "       'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
        "       'radius_se', 'texture_se', 'smoothness_se', 'compactness_se',\n",
        "       'symmetry_se', 'fractal_dimension_se']\n",
        "\n",
        "features_df = cancer_data[features]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_df, targets, test_size=0.3, random_state=40)\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yTwsADfCMD4"
      },
      "source": [
        "**Discriminant Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7n_wLyd9NwY"
      },
      "source": [
        "##Discriminant Analysis\n",
        "\n",
        "def discriminant_binary(X_train, y_train, X_test, y_test):\n",
        "    train = pd.concat([X_train, y_train], axis=1, sort=False)\n",
        "    sigma =  np.cov(X_train.T)\n",
        "    sigma_inv = inv(sigma)\n",
        "    mu_1 = train.loc[train[\"diagnosis\"]==1].mean() \n",
        "    mu_2 = train.loc[train[\"diagnosis\"]==0].mean()\n",
        "    mu_1 = np.array(mu_1.drop(index = \"diagnosis\"))\n",
        "    mu_2 = np.array(mu_2.drop(index = \"diagnosis\"))\n",
        "    p_c1 = np.mean(np.array(train[\"diagnosis\"]))\n",
        "    p_c2 = 1-p_c1\n",
        "    w = np.matmul(sigma_inv,(mu_1-mu_2))\n",
        "    w_0 = -0.5*np.matmul(np.matmul(mu_1.T,sigma_inv),mu_1)+ 0.5*np.matmul(np.matmul(mu_2.T,sigma_inv),mu_2)+np.log(p_c1/p_c2)\n",
        "    train_pred = w_0 + np.matmul(np.array(X_train),w)\n",
        "    train_pred[train_pred > 0] =1\n",
        "    train_pred[train_pred < 0] = 0\n",
        "    correct_train = (train_pred == y_train)\n",
        "    accuracy_train = correct_train.sum() / correct_train.size\n",
        "    test_pred = w_0 + np.matmul(np.array(X_test),w)\n",
        "    test_pred[test_pred > 0] =1\n",
        "    test_pred[test_pred < 0] = 0\n",
        "    correct_test = (test_pred == y_test)    \n",
        "    accuracy_test = correct_test.sum() / correct_test.size\n",
        "    \n",
        "    return (accuracy_test, accuracy_train)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyR3iWQY_y-l",
        "outputId": "3de618cd-3c2b-4006-a4a6-c9f0a83f5adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_acc, train_acc = discriminant_binary(X_train, y_train, X_test, y_test)\n",
        "print(\"Training Accuracy:\"+str(train_acc)) \n",
        "print(\"Test Accuracy:\"+str(test_acc)) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:0.8944723618090452\n",
            "Test Accuracy:0.935672514619883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVBSj_g6CQwu"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydYrYKwmIZtX"
      },
      "source": [
        "#Import Packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Pre Process Data\n",
        "X_train = preprocessing.scale(X_train)\n",
        "X_test = preprocessing.scale(X_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMggustjBZA5"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim,bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "n_epochs = 50\n",
        "n_samples = X_train.shape[0]\n",
        "n_features = X_train.shape[1]\n",
        "n_classes = y_train.value_counts().shape[0]\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ40Qz8QBa-_"
      },
      "source": [
        "#Define the Logistic Regression Function to train the model\n",
        "\n",
        "def LR(X_train, y_train, X_test, y_test, n_epochs, n_features,n_classes,learning_rate):\n",
        "    X, y = torch.from_numpy(np.array(X_train)).float(), torch.from_numpy(np.array(y_train)).long()\n",
        "    Xtest, ytest = torch.from_numpy(np.array(X_test)).float(), torch.from_numpy(np.array(y_test)).long()    \n",
        "    model = LogisticRegressionModel(n_features, n_classes)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate) # Experiment to see the senstivity of learning rate with SGD\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Train\n",
        "    model.train('True')\n",
        "    loss_train = []\n",
        "    loss_test = []\n",
        "    train_list = []\n",
        "    test_list = []\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch number:\"+str(epoch))        \n",
        "        y_hat = model(X)\n",
        "        loss = criterion(y_hat.squeeze(), y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # Backward Pass\n",
        "        optimizer.step()\n",
        "        loss_train.append(loss.item())\n",
        "        #Training Accuracy\n",
        "        _, predicted = torch.max(y_hat.data,1)\n",
        "        correct = (predicted == y) \n",
        "        accuracy = int(correct.sum()) / correct.shape[0]\n",
        "        print(\"Training Accuracy\")\n",
        "        print(accuracy)\n",
        "        train_list.append(accuracy)\n",
        "        #Test Accuracy\n",
        "        y_hat_test = model(Xtest)     \n",
        "        losstest = criterion(y_hat_test.squeeze(), ytest)\n",
        "        loss_test.append(losstest.item())\n",
        "        _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "        correct_test = (predicted_test == ytest)    \n",
        "        accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "        print(\"Test Accuracy\")\n",
        "        print(accuracy_test)\n",
        "        test_list.append(accuracy_test)\n",
        "    return (loss_train, loss_test ,train_list,test_list)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Pl0SR5BbE6",
        "outputId": "62e8deb1-632f-41b9-e8f4-035b590a9167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_train, loss_test, train_list, test_list = LR(X_train, y_train, X_test, y_test, n_epochs, n_features,n_classes,learning_rate)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number:0\n",
            "Training Accuracy\n",
            "0.31155778894472363\n",
            "Test Accuracy\n",
            "0.47368421052631576\n",
            "Epoch number:1\n",
            "Training Accuracy\n",
            "0.41708542713567837\n",
            "Test Accuracy\n",
            "0.5614035087719298\n",
            "Epoch number:2\n",
            "Training Accuracy\n",
            "0.5301507537688442\n",
            "Test Accuracy\n",
            "0.631578947368421\n",
            "Epoch number:3\n",
            "Training Accuracy\n",
            "0.6206030150753769\n",
            "Test Accuracy\n",
            "0.695906432748538\n",
            "Epoch number:4\n",
            "Training Accuracy\n",
            "0.678391959798995\n",
            "Test Accuracy\n",
            "0.7368421052631579\n",
            "Epoch number:5\n",
            "Training Accuracy\n",
            "0.7336683417085427\n",
            "Test Accuracy\n",
            "0.7719298245614035\n",
            "Epoch number:6\n",
            "Training Accuracy\n",
            "0.7763819095477387\n",
            "Test Accuracy\n",
            "0.7953216374269005\n",
            "Epoch number:7\n",
            "Training Accuracy\n",
            "0.8040201005025126\n",
            "Test Accuracy\n",
            "0.8128654970760234\n",
            "Epoch number:8\n",
            "Training Accuracy\n",
            "0.8241206030150754\n",
            "Test Accuracy\n",
            "0.8304093567251462\n",
            "Epoch number:9\n",
            "Training Accuracy\n",
            "0.8442211055276382\n",
            "Test Accuracy\n",
            "0.8362573099415205\n",
            "Epoch number:10\n",
            "Training Accuracy\n",
            "0.864321608040201\n",
            "Test Accuracy\n",
            "0.8421052631578947\n",
            "Epoch number:11\n",
            "Training Accuracy\n",
            "0.8743718592964824\n",
            "Test Accuracy\n",
            "0.8421052631578947\n",
            "Epoch number:12\n",
            "Training Accuracy\n",
            "0.8844221105527639\n",
            "Test Accuracy\n",
            "0.8538011695906432\n",
            "Epoch number:13\n",
            "Training Accuracy\n",
            "0.8894472361809045\n",
            "Test Accuracy\n",
            "0.8654970760233918\n",
            "Epoch number:14\n",
            "Training Accuracy\n",
            "0.8919597989949749\n",
            "Test Accuracy\n",
            "0.8771929824561403\n",
            "Epoch number:15\n",
            "Training Accuracy\n",
            "0.8969849246231156\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:16\n",
            "Training Accuracy\n",
            "0.9045226130653267\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:17\n",
            "Training Accuracy\n",
            "0.9095477386934674\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:18\n",
            "Training Accuracy\n",
            "0.9120603015075377\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:19\n",
            "Training Accuracy\n",
            "0.9195979899497487\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:20\n",
            "Training Accuracy\n",
            "0.9221105527638191\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:21\n",
            "Training Accuracy\n",
            "0.9221105527638191\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:22\n",
            "Training Accuracy\n",
            "0.9221105527638191\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:23\n",
            "Training Accuracy\n",
            "0.9221105527638191\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:24\n",
            "Training Accuracy\n",
            "0.9221105527638191\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:25\n",
            "Training Accuracy\n",
            "0.9221105527638191\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:26\n",
            "Training Accuracy\n",
            "0.9221105527638191\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:27\n",
            "Training Accuracy\n",
            "0.9246231155778895\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:28\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8830409356725146\n",
            "Epoch number:29\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8888888888888888\n",
            "Epoch number:30\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8888888888888888\n",
            "Epoch number:31\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8888888888888888\n",
            "Epoch number:32\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8888888888888888\n",
            "Epoch number:33\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8888888888888888\n",
            "Epoch number:34\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8947368421052632\n",
            "Epoch number:35\n",
            "Training Accuracy\n",
            "0.9271356783919598\n",
            "Test Accuracy\n",
            "0.8947368421052632\n",
            "Epoch number:36\n",
            "Training Accuracy\n",
            "0.9296482412060302\n",
            "Test Accuracy\n",
            "0.8947368421052632\n",
            "Epoch number:37\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.8947368421052632\n",
            "Epoch number:38\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.8947368421052632\n",
            "Epoch number:39\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.8947368421052632\n",
            "Epoch number:40\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.8947368421052632\n",
            "Epoch number:41\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9005847953216374\n",
            "Epoch number:42\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9064327485380117\n",
            "Epoch number:43\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9064327485380117\n",
            "Epoch number:44\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9064327485380117\n",
            "Epoch number:45\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9064327485380117\n",
            "Epoch number:46\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9064327485380117\n",
            "Epoch number:47\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9064327485380117\n",
            "Epoch number:48\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9181286549707602\n",
            "Epoch number:49\n",
            "Training Accuracy\n",
            "0.9321608040201005\n",
            "Test Accuracy\n",
            "0.9181286549707602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz6u5mD3BbHk",
        "outputId": "c529dcd4-7b6c-485b-8a42-d326bb28b785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "##Plot losses\n",
        "\n",
        "plt.plot(list(range(n_epochs)),loss_train, label = \"Training Set\" )\n",
        "plt.plot(list(range(n_epochs)),loss_test, label = \"Test Set\" )\n",
        "plt.title('Cross-Entropy Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jRpaGgDBbKM",
        "outputId": "2e23c675-f998-409c-851e-720ea73a5a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "\n",
        "#Plot Accuracy\n",
        "plt.plot(list(range(n_epochs)),train_list, label = \"Training Set\" )\n",
        "plt.plot(list(range(n_epochs)),test_list, label = \"Test Set\" )\n",
        "plt.title('Evolution of Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAuxfUqRBZrl"
      },
      "source": [
        "**Problem 5: First-order Optimization Methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKnJ_KhxBhRN"
      },
      "source": [
        "import scipy\n",
        "import scipy.io\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aztKHUZxEKya",
        "outputId": "f311047f-66d4-4ef8-b0b7-7e9bffd370d1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e32a9584-ab5d-465a-9dcf-471d8646ab15\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e32a9584-ab5d-465a-9dcf-471d8646ab15\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving mnist-original.mat to mnist-original.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT_D-1RpGpHH",
        "outputId": "4dd9a88a-29be-4b7e-a0bf-b5c3a131ea09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#import Data\n",
        "mnist_data = scipy.io.loadmat('mnist-original.mat')\n",
        "X = mnist_data['data'].T\n",
        "y = mnist_data['label'].T\n",
        "#Pre process\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "#Train and test set\n",
        "train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=1/7, random_state=671)\n",
        "train_labels = pd.get_dummies(pd.DataFrame(train_labels)[0], prefix='numb')\n",
        "test_labels = pd.get_dummies(pd.DataFrame(test_labels)[0], prefix='numb')\n",
        "print (train_dataset.shape, train_labels.shape)\n",
        "print (test_dataset.shape, test_labels.shape)\n",
        "#Get everything in np.array\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "#Use Pytorch\n",
        "X, y = torch.from_numpy(np.array(train_dataset)).float(), torch.from_numpy(np.array(train_labels)).long()\n",
        "Xtest, ytest = torch.from_numpy(np.array(test_dataset)).float(), torch.from_numpy(np.array(test_labels)).long()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784) (60000, 10)\n",
            "(10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzdB3dR3Js87"
      },
      "source": [
        "#Define logistic regression\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim,bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHYBS9bHFRQq"
      },
      "source": [
        "**Momentum Method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1SFaOmNFQri",
        "outputId": "a1eecb96-91d4-4d88-ebdb-4f64e3fc228e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Initialize Model Parameters\n",
        "\n",
        "lambdas = [0.01,0.1,1]\n",
        "learning_rate = 0.001\n",
        "beta = 0.9\n",
        "n_epochs = 200      \n",
        "batch_sizes = [500,60000]    \n",
        "    \n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        m_w = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w = m_w.T\n",
        "        m_b = torch.zeros(y.shape[1])\n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for epoch in list(range(n_epochs))[1:]:\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                b = list(model.parameters())[1]\n",
        "                w = list(model.parameters())[0]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat.squeeze(), torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w.grad is not None:\n",
        "                    w.grad.zero_()\n",
        "                if b.grad is not None:\n",
        "                    b.grad.zero_()\n",
        "                loss.backward()\n",
        "                with torch.no_grad():  # 3\n",
        "                    m_w = beta*m_w + w.grad\n",
        "                    m_b = beta*m_b +b.grad\n",
        "                    w -= learning_rate * m_w\n",
        "                    b -= learning_rate * m_b\n",
        "                #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (epoch+1)%(5*10)==0:    \n",
        "                print(\"Epoch number:\"+str(epoch+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:500\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.89\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.898\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.9019\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.9044\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8787\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8815\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.882\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8815\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.7219\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.6768\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.7196\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.647\n",
            "BATCH SIZE:60000\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.6321\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.7471\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.7808\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.7977\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.6254\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.7342\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.7695\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.786\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.6346\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.7418\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.7685\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.7748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er8pu4ndXbmk",
        "outputId": "94828e19-bed8-45a8-8e80-bc680a75515d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "#Initialize Model Parameters\n",
        "\n",
        "lambdas = [0.01,0.1,1]\n",
        "learning_rate = 0.001\n",
        "beta = 0.9\n",
        "n_epochs = 2      \n",
        "batch_sizes = [1]    \n",
        "    \n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        m_w = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w = m_w.T\n",
        "        m_b = torch.zeros(y.shape[1])\n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for epoch in list(range(n_epochs))[1:]:\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                b = list(model.parameters())[1]\n",
        "                w = list(model.parameters())[0]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat, torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w.grad is not None:\n",
        "                    w.grad.zero_()\n",
        "                if b.grad is not None:\n",
        "                    b.grad.zero_()\n",
        "                loss.backward()\n",
        "                with torch.no_grad():  # 3\n",
        "                    m_w = beta*m_w + w.grad\n",
        "                    m_b = beta*m_b +b.grad\n",
        "                    w -= learning_rate * m_w\n",
        "                    b -= learning_rate * m_b\n",
        "                #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (epoch+1)%(1)==0:    \n",
        "                print(\"Epoch number:\"+str(epoch+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:1\n",
            "LAMBDA:0.01\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.8877\n",
            "LAMBDA:0.1\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.8671\n",
            "LAMBDA:1\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.4762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9NtmWcJwZq"
      },
      "source": [
        "**NAG Method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KFQ810GJwso",
        "outputId": "3dc84314-77cf-4666-b829-1e4d7eefde17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lambdas = [0.01,0.1,1]\n",
        "learning_rate = 0.001\n",
        "beta = 0.95\n",
        "n_epochs = 200    \n",
        "batch_sizes = [500,60000]    \n",
        "    \n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        m_w = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w = m_w.T\n",
        "        m_b = torch.zeros(y.shape[1]) \n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for epoch in list(range(n_epochs))[1:]:\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                w = list(model.parameters())[0]\n",
        "                b = list(model.parameters())[1]\n",
        "                w = w +beta*m_w\n",
        "                b = b +beta*m_b\n",
        "                y_hat = model(batch_x)\n",
        "                w_fake = list(model.parameters())[0]\n",
        "                b_fake = list(model.parameters())[1]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat.squeeze(), torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w_fake.grad is not None:\n",
        "                    w_fake.grad.zero_()\n",
        "                if b_fake.grad is not None:\n",
        "                    b_fake.grad.zero_()    \n",
        "                loss.backward()\n",
        "                \n",
        "                with torch.no_grad():  # 3\n",
        "                    m_w = beta*m_w - learning_rate*w_fake.grad\n",
        "                    m_b = beta*m_b - learning_rate*b_fake.grad\n",
        "                    w +=  m_w\n",
        "                    b +=  m_b\n",
        "                #Update parameters\n",
        "                model.linear.weight.data = w.clone()\n",
        "                model.linear.bias.data = b.clone()\n",
        "            #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (epoch+1)%(5*10)==0:    \n",
        "                print(\"Epoch number:\"+str(epoch+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:500\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.9039\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.9074\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.9094\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.9106\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8818\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8807\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8816\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8802\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.657\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.7273\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.6025\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.6721\n",
            "BATCH SIZE:60000\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.7596\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8137\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8324\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8439\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.769\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8155\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8321\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8407\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.7641\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.7854\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.7533\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.7355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdBCGd3DXuZF",
        "outputId": "1e0f0846-76d6-44d2-807f-ffdfee9ed529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "lambdas = [0.01,0.1,1]\n",
        "learning_rate = 0.001\n",
        "beta = 0.95\n",
        "n_epochs = 2    \n",
        "batch_sizes = [1]    \n",
        "    \n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        m_w = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w = m_w.T\n",
        "        m_b = torch.zeros(y.shape[1]) \n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for epoch in list(range(n_epochs))[1:]:\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                w = list(model.parameters())[0]\n",
        "                b = list(model.parameters())[1]\n",
        "                w = w +beta*m_w\n",
        "                b = b +beta*m_b\n",
        "                y_hat = model(batch_x)\n",
        "                w_fake = list(model.parameters())[0]\n",
        "                b_fake = list(model.parameters())[1]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat, torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w_fake.grad is not None:\n",
        "                    w_fake.grad.zero_()\n",
        "                if b_fake.grad is not None:\n",
        "                    b_fake.grad.zero_()    \n",
        "                loss.backward()\n",
        "                \n",
        "                with torch.no_grad():  # 3\n",
        "                    m_w = beta*m_w - learning_rate*w_fake.grad\n",
        "                    m_b = beta*m_b - learning_rate*b_fake.grad\n",
        "                    w +=  m_w\n",
        "                    b +=  m_b\n",
        "                #Update parameters\n",
        "                model.linear.weight.data = w.clone()\n",
        "                model.linear.bias.data = b.clone()\n",
        "            #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (epoch+1)%(1)==0:    \n",
        "                print(\"Epoch number:\"+str(epoch+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:1\n",
            "LAMBDA:0.01\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.889\n",
            "LAMBDA:0.1\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.8059\n",
            "LAMBDA:1\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.5323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4QSUyzJ1sj"
      },
      "source": [
        "**RMSprop Method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xEh2RTeJ5Qe",
        "outputId": "11d86efb-6cbd-4b11-faae-16f6eb1a05d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Initialize Model Parameters\n",
        "lambdas = [0.01,0.1,1]\n",
        "learning_rate = 0.001\n",
        "beta = 0.9\n",
        "gamma = 1\n",
        "epsilon = 10**(-8)\n",
        "n_epochs = 200     \n",
        "batch_sizes = [500,60000]    \n",
        "    \n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        v_w = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        v_w = v_w.T\n",
        "        v_b = torch.zeros(y.shape[1])    \n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for epoch in list(range(n_epochs))[1:]:\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                b = list(model.parameters())[1]\n",
        "                w = list(model.parameters())[0]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat.squeeze(), torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w.grad is not None:\n",
        "                    w.grad.zero_()\n",
        "                if b.grad is not None:\n",
        "                    b.grad.zero_()\n",
        "                loss.backward()\n",
        "                with torch.no_grad():  # 3\n",
        "                    v_w = (1-beta)*v_w + beta*(w.grad*w.grad)\n",
        "                    v_b = (1-beta)*v_b + beta*(b.grad*b.grad)\n",
        "                    b_w = torch.div(gamma, epsilon + torch.sqrt(v_w)) \n",
        "                    b_b = torch.div(gamma, epsilon + torch.sqrt(v_b))\n",
        "                    w -= learning_rate * (w.grad*b_w)\n",
        "                    b -= learning_rate * (b.grad*b_b)\n",
        "            #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (epoch+1)%(5*10)==0:\n",
        "                print(\"Epoch number:\"+str(epoch+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:500\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.9105\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.9098\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.9089\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.9095\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8777\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8796\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8807\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8792\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.6244\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.7023\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.6971\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.5893\n",
            "BATCH SIZE:60000\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8183\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8702\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8894\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8993\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8209\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8652\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8752\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.879\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.6877\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.6578\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.6474\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.6444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfeNhKLqYRob",
        "outputId": "24497d85-3879-40cd-e00d-eb5053ff62e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "#Initialize Model Parameters\n",
        "lambdas = [0.01,0.1,1]\n",
        "learning_rate = 0.001\n",
        "beta = 0.9\n",
        "gamma = 1\n",
        "epsilon = 10**(-8)\n",
        "n_epochs = 2    \n",
        "batch_sizes = [1]    \n",
        "    \n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        v_w = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        v_w = v_w.T\n",
        "        v_b = torch.zeros(y.shape[1])    \n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for epoch in list(range(n_epochs))[1:]:\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                b = list(model.parameters())[1]\n",
        "                w = list(model.parameters())[0]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat, torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w.grad is not None:\n",
        "                    w.grad.zero_()\n",
        "                if b.grad is not None:\n",
        "                    b.grad.zero_()\n",
        "                loss.backward()\n",
        "                with torch.no_grad():  # 3\n",
        "                    v_w = (1-beta)*v_w + beta*(w.grad*w.grad)\n",
        "                    v_b = (1-beta)*v_b + beta*(b.grad*b.grad)\n",
        "                    b_w = torch.div(gamma, epsilon + torch.sqrt(v_w)) \n",
        "                    b_b = torch.div(gamma, epsilon + torch.sqrt(v_b))\n",
        "                    w -= learning_rate * (w.grad*b_w)\n",
        "                    b -= learning_rate * (b.grad*b_b)\n",
        "            #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (epoch+1)%(1)==0:\n",
        "                print(\"Epoch number:\"+str(epoch+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)\n",
        "    "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:1\n",
            "LAMBDA:0.01\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.7666\n",
            "LAMBDA:0.1\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.7472\n",
            "LAMBDA:1\n",
            "Epoch number:2\n",
            "Test Accuracy\n",
            "0.1129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQo-LisAmlW6"
      },
      "source": [
        "**ADAM Method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu-RbZJPtOxw",
        "outputId": "c6770d26-0b39-4eaf-9034-9a242feb201b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Initialize Model Parameters\n",
        "learning_rate = 0.001\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 10**(-8)\n",
        "n_epochs = 200\n",
        "lambdas = [0.01,0.1,1]\n",
        "batch_sizes = [500,60000]    \n",
        "\n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        m_w_1 = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w_1 = m_w_1.T\n",
        "        m_w_2 = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w_2 = m_w_2.T\n",
        "        m_b_1 = torch.zeros(y.shape[1])  \n",
        "        m_b_2 = torch.zeros(y.shape[1])   \n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for k in list(range(n_epochs))[1:]:\n",
        "            k +=1\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                b = list(model.parameters())[1]\n",
        "                w = list(model.parameters())[0]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat.squeeze(), torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w.grad is not None:\n",
        "                    w.grad.zero_()\n",
        "                if b.grad is not None:\n",
        "                    b.grad.zero_()\n",
        "                loss.backward()\n",
        "                with torch.no_grad():\n",
        "                    m_w_1 = beta_1*m_w_1 + (1-beta_1)*w.grad\n",
        "                    m_b_1 = beta_1*m_b_1 + (1-beta_1)*b.grad\n",
        "                    m_w_tilde_1 = m_w_1*(1-beta_1**(k-1))**(-1)        \n",
        "                    m_b_tilde_1 = m_b_1*(1-beta_1**(k-1))**(-1)        \n",
        "                    m_w_2 = beta_2*m_w_2 + (1-beta_2)*(w.grad*w.grad)\n",
        "                    m_b_2 = beta_2*m_b_2 + (1-beta_2)*(b.grad*b.grad)        \n",
        "                    m_w_tilde_2 = m_w_2*(1-beta_2**(k-1))**(-1)        \n",
        "                    m_b_tilde_2 = m_b_2*(1-beta_2**(k-1))**(-1)      \n",
        "                    w -= learning_rate*torch.div(m_w_tilde_1, epsilon + torch.sqrt(m_w_tilde_2))\n",
        "                    b -= learning_rate*torch.div(m_b_tilde_1, epsilon + torch.sqrt(m_b_tilde_2))\n",
        "            #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (k+1)%(5*10)==0:       \n",
        "                print(\"Epoch number:\"+str(k+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:500\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.9086\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.9107\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.9109\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.911\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8809\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8813\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8813\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8812\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.7131\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.6534\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.6457\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.681\n",
            "BATCH SIZE:60000\n",
            "LAMBDA:0.01\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8078\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8533\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8716\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.881\n",
            "LAMBDA:0.1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.8102\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.8483\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.8611\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.8677\n",
            "LAMBDA:1\n",
            "Epoch number:50\n",
            "Test Accuracy\n",
            "0.764\n",
            "Epoch number:100\n",
            "Test Accuracy\n",
            "0.7362\n",
            "Epoch number:150\n",
            "Test Accuracy\n",
            "0.7143\n",
            "Epoch number:200\n",
            "Test Accuracy\n",
            "0.7131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8abrgXsxY5K0",
        "outputId": "e8871556-33f3-40bd-e5b8-c1776ad07c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "#Initialize Model Parameters\n",
        "learning_rate = 0.001\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 10**(-8)\n",
        "n_epochs = 2\n",
        "lambdas = [0.01,0.1,1]\n",
        "batch_sizes = [1]    \n",
        "\n",
        "#Estimation \n",
        "for batch_size in batch_sizes:\n",
        "    print(\"BATCH SIZE:\"+str(batch_size))\n",
        "    for lambda_param in lambdas:\n",
        "        model = LogisticRegressionModel(X.shape[1], y.shape[1])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        m_w_1 = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w_1 = m_w_1.T\n",
        "        m_w_2 = torch.zeros((X.shape[1],y.shape[1] ))\n",
        "        m_w_2 = m_w_2.T\n",
        "        m_b_1 = torch.zeros(y.shape[1])  \n",
        "        m_b_2 = torch.zeros(y.shape[1])   \n",
        "        print(\"LAMBDA:\"+str(lambda_param))\n",
        "        for k in list(range(n_epochs))[1:]:\n",
        "            k +=1\n",
        "             # X is a torch Variable\n",
        "            permutation = torch.randperm(X.size()[0])\n",
        "            for i in range(0,X.size()[0], batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch_x, batch_y = X[indices], y[indices]\n",
        "                y_hat = model(batch_x)\n",
        "                b = list(model.parameters())[1]\n",
        "                w = list(model.parameters())[0]\n",
        "                l2_regularization = torch.norm(b) + torch.norm(w)\n",
        "                loss = criterion(y_hat, torch.max(batch_y, 1)[1]) + lambda_param*l2_regularization\n",
        "                if w.grad is not None:\n",
        "                    w.grad.zero_()\n",
        "                if b.grad is not None:\n",
        "                    b.grad.zero_()\n",
        "                loss.backward()\n",
        "                with torch.no_grad():\n",
        "                    m_w_1 = beta_1*m_w_1 + (1-beta_1)*w.grad\n",
        "                    m_b_1 = beta_1*m_b_1 + (1-beta_1)*b.grad\n",
        "                    m_w_tilde_1 = m_w_1*(1-beta_1**(k-1))**(-1)        \n",
        "                    m_b_tilde_1 = m_b_1*(1-beta_1**(k-1))**(-1)        \n",
        "                    m_w_2 = beta_2*m_w_2 + (1-beta_2)*(w.grad*w.grad)\n",
        "                    m_b_2 = beta_2*m_b_2 + (1-beta_2)*(b.grad*b.grad)        \n",
        "                    m_w_tilde_2 = m_w_2*(1-beta_2**(k-1))**(-1)        \n",
        "                    m_b_tilde_2 = m_b_2*(1-beta_2**(k-1))**(-1)      \n",
        "                    w -= learning_rate*torch.div(m_w_tilde_1, epsilon + torch.sqrt(m_w_tilde_2))\n",
        "                    b -= learning_rate*torch.div(m_b_tilde_1, epsilon + torch.sqrt(m_b_tilde_2))\n",
        "            #Test Accuracy\n",
        "            y_hat_test = model(Xtest)     \n",
        "            _, predicted_test = torch.max(y_hat_test.data,1)\n",
        "            correct_test = (predicted_test == torch.max(ytest, 1)[1])    \n",
        "            accuracy_test = int(correct_test.sum()) / correct_test.shape[0]\n",
        "            if (k+1)%(1)==0:       \n",
        "                print(\"Epoch number:\"+str(k+1))\n",
        "                print(\"Test Accuracy\")\n",
        "                print(accuracy_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:1\n",
            "LAMBDA:0.01\n",
            "Epoch number:3\n",
            "Test Accuracy\n",
            "0.9028\n",
            "LAMBDA:0.1\n",
            "Epoch number:3\n",
            "Test Accuracy\n",
            "0.8674\n",
            "LAMBDA:1\n",
            "Epoch number:3\n",
            "Test Accuracy\n",
            "0.3633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8ybzWXkuXW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWjzOgKa0Oi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}